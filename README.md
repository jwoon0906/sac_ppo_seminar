# sac_ppo_seminar

##개요
이 레포지토리는 강화학습(RL) 알고리즘의 발전 계보 속에서
Proximal Policy Optimization (PPO)와 Soft Actor-Critic(SAC)을 직관적으로 이해하기 위한 발표 자료입니다.
수식보다는, Value-based -> Policy-based -> Actor-Critic -> PPO로 이어지는 직관적 흐름과 역사적 맥락 중심으로 설명하였습니다.

##목적
* 강화학습 알고리즘의 발전 방향을 큰 그림에서 조망
* Q-Learning, DQN -> Policy Gradient -> Actor-Critic -> PPO로 이어지는 자연스러운 계보 이해
* 각 단계가 해결하려 했던 문제와 한계 파악


